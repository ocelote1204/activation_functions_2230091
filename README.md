Activation Functions Visualization





Este repositorio contiene una implementación en Python para visualizar funciones de activación comunes utilizadas en redes neuronales, junto con sus derivadas.

📌 Contenido del Proyecto

src/main.py: Script principal que genera gráficos de funciones de activación.

requirements.txt: Lista de dependencias necesarias para ejecutar el proyecto.

.gitignore: Archivos y carpetas ignorados por Git.

README.md: Documentación del proyecto.

📊 Funciones de Activación Implementadas

ReLU (Rectified Linear Unit) y su derivada.

Sigmoide y su derivada.

Tanh (Tangente Hiperbólica) y su derivada.

Leaky ReLU y su derivada.

ELU (Exponential Linear Unit) y su derivada.

🚀 Instalación y Uso

1️ Clonar el repositorio

 git clone https://github.com/ocelote1204/activation_functions_2230091.git
 cd activation_functions_2230091

2️ Crear y activar un entorno virtual

 python -m venv venv

En Windows:

venv\Scripts\activate

En macOS/Linux:

source venv/bin/activate

3️Instalar las dependencias

 pip install -r requirements.txt

4️Ejecutar el script

 python src/main.py

🖼️ Ejemplo de Salida

El código generará gráficos como este, mostrando la función de activación y su derivada:



📌 Estructura del Repositorio

activation_functions_2230091/
│── src/
│   ├── main.py  # Código principal para graficar funciones de activación
│   ├── requirements.txt  # Dependencias del proyecto
│   ├── README.md    # Documentación

🛠️ Dependencias

Python 3.10+

NumPy 2.2.3

Matplotlib 3.10.0

📜 Licencia

Este proyecto está bajo la licencia MIT. Puedes usarlo, modificarlo y distribuirlo libremente.

💡 Desarrollado por Osmar Amir Sanchez Gomez

 
