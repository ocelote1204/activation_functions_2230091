Activation Functions Visualization

This repository contains a Python implementation to visualize common activation functions used in neural networks, along with their derivatives.

📌 Project Content

src/main.py: Main script that generates plots of activation functions.
requirements.txt: List of dependencies required to run the project.
.gitignore: Files and folders ignored by Git.
README.md: Project documentation.
📊 Implemented Activation Functions

ReLU (Rectified Linear Unit) and its derivative.
Sigmoid and its derivative.
Tanh (Hyperbolic Tangent) and its derivative.
Leaky ReLU and its derivative.
ELU (Exponential Linear Unit) and its derivative.
🚀 Installation and Usage

1️ Clone the repository:

🚀 Installation and Usage

1️ Clone the repository:

 git clone https://github.com/ocelote1204/activation_functions_2230091.git
 cd activation_functions_2230091

2️ Create and activate a virtual environment:

 python -m venv venv

On Windows:

venv\Scripts\activate

On macOS/Linux:

source venv/bin/activate

3️ Install dependencies:

 pip install -r requirements.txt

4️ Run the script:

 python src/main.py


📌 Repository Structure

activation_functions_2230091/
│── src/
│   ├── main.py       # Main code for plotting activation functions
│   ├── requirements.txt  # Project dependencies
│   ├── README.md    # Documentation

🛠️ Dependencies

Python 3.10+
NumPy 2.2.3
Matplotlib 3.10.0

📜 License

You can use, modify, and distribute it freely.

💡 Developed by Osmar Amir Sanchez Gomez
 
